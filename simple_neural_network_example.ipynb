{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SpiEuBqDXJPC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzTg_7szXJPJ"
   },
   "source": [
    "# Evolving a simple neural network\n",
    "\n",
    "In this notebook we will evolve a simple single layer perceptron while walking through all components of a genetic algorithm:\n",
    "\n",
    "<ul>\n",
    "<li>representation (definition of individuals)</li>\n",
    "<li>evaluation function (or fitness function)</li>\n",
    "<li>population</li>\n",
    "<li>parent selection mechanism</li>\n",
    "<li>variation operators, recombination and mutation</li>\n",
    "<li>survivor selection mechanism (replacement)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTX0mCk9XJPL"
   },
   "source": [
    "## Representation\n",
    "\n",
    "Solution representation is highly dependent on the problem. In this case a simple control task Mountain Car from openAI gym. \n",
    "The example below shows an agent taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-X5eoNZcXJPL"
   },
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample () \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    if terminated:\n",
    "        env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk_MA9CrXJPM"
   },
   "source": [
    "The goal of our agent is to reach the flag on top of the hill. To achieve this it must make use of the observations of the environment to decide the correct action. \n",
    "\n",
    "Our first step is to discover the nature and scope of both these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yo_O8VOEXJPM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "The action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "# Observation and action space \n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC8z7faWXJPN"
   },
   "source": [
    "From this we can see that the observation space is a two-dimensional continuous vector. The values will represent the position and velocity of the cart.\n",
    "\n",
    "As action the agent has a choice of 3 discrete options: applying force in either of the directions or applying no force.\n",
    "\n",
    "A possible mapping from observation to action is a single-layer perceptron with an output node for every distinct action. After activation we can simply take the action with the highest corresponding output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7gLRA8lrXJPN"
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "    def activation(self, input, weights):\n",
    "        x = np.dot(input, weights.reshape (self.n_inputs, self.n_outputs))\n",
    "        return np.argmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4ZgtFKjXJPO"
   },
   "source": [
    "This brings us to the representation of an individual solution. Remember that we only want to store necessary information into the genotype. So, since the structure of our network will remain static, the only things that are subject to change are the weight values. This way we can compactly store our individuals as an array of weights. \n",
    "\n",
    "Let's take a look how this all comes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lAhLDyyjXJPO"
   },
   "outputs": [],
   "source": [
    "individual = np.array ([0.1, -0.1, 0, 0.5, -0.5, 0.3], dtype=np.float64)\n",
    "\n",
    "network = Perceptron (2, 3)\n",
    "\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = network.activation (observation, individual) \n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A52YPabsXJPO"
   },
   "source": [
    "Okay that seems worse than taking random actions but that is where our algorithm will come into play.\n",
    "\n",
    "Although in this case the solution is fairly simple so feel free to play around with the weights to see if you can find a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOU-lO_SXJPP"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Of course we need a way to quantify an individual's performance. Luckily gym already provides a reward signal on each timestep so we can sum these and return the total. We also remove the render call to speed up the process significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DNI9XZ0-XJPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1000.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate(weights, net, env):\n",
    "    observation, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(1000):\n",
    "        action = net.activation(observation, weights)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated:\n",
    "            observation = env.reset()\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "print (evaluate (individual, network, env))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-Km0ebvXJPP"
   },
   "source": [
    "## Population\n",
    "\n",
    "Let's create a population. For this we only need to define some bounds for our weight values. In this case we generate the values over an uniform distribution. we can be more efficient by storing everything in a two-dimensional array with shape N_Population x N_Weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g0mkOBrBXJPP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82725159,  1.17104059, -1.21178328, -1.98777683,  1.25517615,\n",
       "         1.86807437],\n",
       "       [-1.43799712, -0.39916722, -0.27659301, -1.5402662 , -1.46197859,\n",
       "         1.61771317],\n",
       "       [-0.65160765, -1.91521983,  0.46231051,  1.93810404,  0.15228484,\n",
       "         1.8194858 ],\n",
       "       [-1.95821316,  0.61916238, -0.76909541,  0.3759587 ,  0.35801913,\n",
       "        -0.85431231],\n",
       "       [-1.0590136 ,  0.71050654, -1.79866446,  0.41521875, -0.36183445,\n",
       "         0.09877404],\n",
       "       [-0.09464154, -0.34034492,  0.81912758,  1.61784611,  1.76119568,\n",
       "        -0.8653237 ],\n",
       "       [-1.88452029, -0.4033983 , -1.17775894,  0.3660249 , -1.90004989,\n",
       "         1.01340749],\n",
       "       [-1.00475774,  0.12747229,  0.6040468 ,  1.4123086 ,  0.27196112,\n",
       "        -0.872508  ],\n",
       "       [ 0.02581191,  0.84793493,  1.40076604, -1.36118284, -0.65963767,\n",
       "        -1.6303648 ],\n",
       "       [ 1.14871067,  0.89745063, -0.34276903, -1.73766896, -1.34227336,\n",
       "        -0.23316449]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_population(population_size, lower, upper, n_weights = 6):\n",
    "    return np.random.uniform(lower, upper, (population_size, n_weights))\n",
    "\n",
    "pop = initialize_population(10, -2, 2)\n",
    "pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVIJsmjPXJPQ"
   },
   "source": [
    "Of course we also want a function to evaluate the entire population. Since in many cases you will be dealing with a noisy environment, let's also add a number of evaluations parameter where we take the mean of all evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AyoHhy4VXJPQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1000. -1000. -1000. -1000. -1000. -1000. -1000. -1000. -1000. -1000.]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_population(pop, number_of_evaluations, net, env):\n",
    "    population_fitness = np.zeros(pop.shape[0])\n",
    "\n",
    "    for i in range (pop.shape [0]):\n",
    "        population_fitness[i] = np.mean ([evaluate (pop[i], net, env) for _ in range (number_of_evaluations)])\n",
    "\n",
    "    return population_fitness\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "print (evaluate_population(pop, 10, network, env))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuChULC1XJPQ"
   },
   "source": [
    "## Parent Selection\n",
    "\n",
    "Now we have a population and a way to evaluate it, we can decide which individuals are fit to be a parent. Often this is done in a stochastic manner but influenced by the individuals fitness score.\n",
    "\n",
    "We want to perform crossover later so we are picking parents in pairs of two.\n",
    "\n",
    "Fitness is recalculated by adding the lowest score to make the value range from 0 upwards. A smoothing factor is added to give the worst individual still a chance to be picked and preventing a divide by 0 in case all individuals have the same fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BEVcIPZHXJPQ"
   },
   "outputs": [],
   "source": [
    "def parent_selection(pop, pop_fit, n_parents, smoothing = 1):\n",
    "    fitness  = pop_fit + smoothing - np.min(pop_fit)\n",
    "\n",
    "    # Fitness proportional selection probability\n",
    "    fps = fitness / np.sum (fitness)\n",
    "    \n",
    "    # make a random selection of indices\n",
    "    parent_indices = np.random.choice (np.arange(0,pop.shape[0]), (n_parents,2), p=fps)\n",
    "    return pop [parent_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p91Z1M8eXJPQ"
   },
   "source": [
    "## Variation\n",
    "\n",
    "The chosen parents create new individuals through crossover. The below example shows the simplest form of crossover where every individual weight is set randomly to one of the parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Nud06Q-7XJPR"
   },
   "outputs": [],
   "source": [
    "def crossover(parents):\n",
    "    parentsA, parentsB = np.hsplit (parents,2)\n",
    "    roll = np.random.uniform (size = parentsA.shape)\n",
    "    offspring = parentsA * (roll >= 0.5) + parentsB * (roll < 0.5)\n",
    "    # squeeze to get rid of the extra dimension created during parent selecting\n",
    "    return np.squeeze(offspring,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXjfHXzIXJPR"
   },
   "source": [
    "Mutation introduces additional variation, in this case we apply a Gaussian mutation on all weights. The Sigma parameter controls the standard deviation with which we can set the average scale of the mutations.\n",
    "\n",
    "Finally we need to ensure that weight values don't go out of bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iYMBLAL9XJPR"
   },
   "outputs": [],
   "source": [
    "def mutate(pop,min_value,max_value, sigma):\n",
    "    mutation = np.random.normal(0, sigma, size=pop.shape)\n",
    "    new_pop = pop + mutation\n",
    "    new_pop[new_pop>max_value] = max_value\n",
    "    new_pop[new_pop<min_value] = min_value\n",
    "    return new_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IelZPobeXJPR"
   },
   "source": [
    "## Survivor Selection\n",
    "\n",
    "After offspring has been generated we need to decide which individuals stay in the population. In contrast to parent selection this is often done deterministically. The example simply picks the top $populationSize$ individuals.\n",
    "\n",
    "To prevent having to reevaluate we also keep the fitness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MR16lxH8XJPR"
   },
   "outputs": [],
   "source": [
    "def survivor_selection(pop, pop_fit, n_pop):\n",
    "    best_fit_indices = np.argsort(pop_fit * -1) # -1 since we are maximizing\n",
    "    survivor_indices = best_fit_indices [:n_pop]\n",
    "    return pop [survivor_indices], pop_fit[survivor_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYAYhS8bXJPR"
   },
   "source": [
    "## Full Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qk7o512kXJPR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen 9 - Best: -116.33333333333333 - Mean: -691.0533333333333\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "population_size = 50\n",
    "n_evaluations = 3\n",
    "n_offspring = 50\n",
    "weight_upper_bound = 2\n",
    "weight_lower_bound = -2\n",
    "mutation_sigma = .1\n",
    "generations = 10\n",
    "\n",
    "# Initialize environment, network and population. Perform an initial evaluation\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "net = Perceptron (2, 3)\n",
    "pop = initialize_population(population_size, weight_lower_bound, weight_upper_bound)\n",
    "pop_fit = evaluate_population(pop, n_evaluations, net, env)\n",
    "\n",
    "for i in range (generations):\n",
    "    parents = parent_selection(pop, pop_fit, n_offspring)\n",
    "    offspring = crossover (parents)\n",
    "    offspring = mutate (offspring, weight_lower_bound, weight_upper_bound, mutation_sigma)\n",
    "\n",
    "    offspring_fit = evaluate_population(offspring, n_evaluations, net, env)\n",
    "\n",
    "    # concatenating to form a new population\n",
    "    pop = np.vstack((pop,offspring))\n",
    "    pop_fit = np.concatenate([pop_fit,offspring_fit])\n",
    "\n",
    "    pop, pop_fit = survivor_selection(pop, pop_fit, population_size)\n",
    "    \n",
    "    print (f\"Gen {i} - Best: {np.max (pop_fit)} - Mean: {np.mean(pop_fit)}\")\n",
    "    clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8FlpObxXJPS"
   },
   "source": [
    "## Notes\n",
    "\n",
    "We can take a look at the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBZJglXBXJPS"
   },
   "outputs": [],
   "source": [
    "individual = pop [np.argmax(pop_fit)]\n",
    "\n",
    "network = Perceptron (2, 3)\n",
    "\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = network.activation (observation, individual) \n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzRmTciWXJPS"
   },
   "source": [
    "If you did not change the parameters there's a good chance that the best found network does not succeed in the task every single time. This could have been caused by many things: \n",
    "\n",
    "Did we not evaluate enough times and did this solution get lucky? \n",
    "\n",
    "Did our search get stuck in a local optimum? \n",
    "\n",
    "Does the algorithm just need more generations to run or a larger population size?\n",
    "\n",
    "In the end it will be up to you to find the answers to these questions. Usually you don't get the answers by running your algorithm just once. You need multiple runs with the same parameters before comfortably making statements about your approach. For this you want to collect as much data from your runs as possible. Below an example for saving data during the run. The class can be extended of course, think about things like measuring diversity within your population during the course of a run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBmh-efdXJPS"
   },
   "outputs": [],
   "source": [
    "class DataGatherer:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.mean_fitness = np.array ([])\n",
    "        self.best_fitness = np.array ([])\n",
    "        self.generations = np.array ([])\n",
    "        self.stats = []\n",
    "\n",
    "        if not os.path.exists(name):\n",
    "            os.mkdir(name)\n",
    "            os.mkdir(name+\"/best\")\n",
    "\n",
    "    def gather(self, pop, pop_fit, gen):\n",
    "        self.mean_fitness = np.concatenate([self.mean_fitness, [np.mean (pop_fit)]])\n",
    "        self.best_fitness = np.concatenate([self.best_fitness, [np.max (pop_fit)]])\n",
    "        self.generations = np.concatenate([self.generations, [gen]])\n",
    "\n",
    "        self.stats = np.stack([self.generations, self.mean_fitness,self.best_fitness])\n",
    "\n",
    "        np.savetxt(f\"{self.name}/stats.out\", self.stats.T, delimiter=',',fmt='%1.2e')\n",
    "        np.savetxt(f\"{self.name}/best/{gen}.out\", pop [np.argmax(pop_fit)],  delimiter=',',fmt='%1.2e')   \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HoM2uXzXJPS"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "population_size = 100\n",
    "n_evaluations = 3\n",
    "n_offspring = 100\n",
    "weight_upper_bound = 2\n",
    "weight_lower_bound = -2\n",
    "mutation_sigma = .1\n",
    "generations = 10\n",
    "\n",
    "# Initialize environment, network and population. Perform an initial evaluation\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "net = Perceptron (2, 3)\n",
    "pop = initialize_population(population_size, weight_lower_bound, weight_upper_bound)\n",
    "pop_fit = evaluate_population(pop, n_evaluations, net, env)\n",
    "data = DataGatherer (\"simple_example\") # think of a good naming convention\n",
    "\n",
    "for gen in range (generations):\n",
    "    parents = parent_selection(pop, pop_fit, n_offspring)\n",
    "    offspring = crossover (parents)\n",
    "    offspring = mutate (offspring, weight_lower_bound, weight_upper_bound, mutation_sigma)\n",
    "\n",
    "    offspring_fit = evaluate_population(offspring, n_evaluations, net, env)\n",
    "\n",
    "    # concatenating to form a new population\n",
    "    pop = np.vstack((pop,offspring))\n",
    "    pop_fit = np.concatenate([pop_fit,offspring_fit])\n",
    "\n",
    "    pop, pop_fit = survivor_selection(pop, pop_fit, population_size)\n",
    "    \n",
    "    data.gather (pop, pop_fit, gen)\n",
    "    print (f\"Gen {gen} - Best: {np.max (pop_fit)} - Mean: {np.mean(pop_fit)}\")\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62BM8ZLUXJPS"
   },
   "source": [
    "We can look at the data with for example plotly and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMA6olEBXJPS"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oCP__n4XJPT"
   },
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt(\"simple_example/stats.out\",delimiter=\",\")\n",
    "data_plot = pd.DataFrame(raw_data, columns=[\"Generation\", \"Mean\", \"Best\"]) \n",
    "px.line (data_plot, x=\"Generation\", y=[\"Mean\", \"Best\"], labels={\"value\": \"Performance\"})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "f63bc2624baaa119f4cb811cccc6a3b26066ee62ed4d098c272801e07cc617dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
